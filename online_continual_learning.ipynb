{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**\n",
        "\n",
        "The goal of this notebook is to see how mutual information develops in an online continual learner. We will be using the MNIST dataset, broken down into different tasks."
      ],
      "metadata": {
        "id": "rjWcyfuhlexr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iohMoDmplXmd"
      },
      "outputs": [],
      "source": [
        "#@title Import Modules\n",
        "import math\n",
        "import functools\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import TensorDataset\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Hyperparameters\n",
        "device        = 'cuda'\n",
        "# Experiment parameters\n",
        "pretrain      = True\n",
        "tasks         = [[0,1], [2,3], [4,5], [6,7], [8,9]]\n",
        "numOutput     = len(sum(tasks, []))\n",
        "if pretrain:\n",
        "    epochs    = 10\n",
        "    numMI     = epochs + len(tasks) - 1\n",
        "else:\n",
        "    epochs    = 1\n",
        "    numMI     = len(tasks)\n",
        "# Pretraining parameters\n",
        "lr            = 7e-2\n",
        "milestones    = [epochs//2]\n",
        "gamma         = 0.5\n",
        "batch_size    = 512\n",
        "momentum      = 0\n",
        "weight_decay  = 0\n",
        "# Online learning parameters\n",
        "lr_ol         = (1/np.arange(1, len(tasks)+1)) * 4e-4\n",
        "print(lr_ol)\n",
        "# Measurement variables\n",
        "n_bins        = 10"
      ],
      "metadata": {
        "id": "2rBft2-HlfGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4c285a-5e10-439e-ccdd-a85f2bb3d554"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.00000000e-04 2.00000000e-04 1.33333333e-04 1.00000000e-04\n",
            " 8.00000000e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca_components = 12\n",
        "\n",
        "#@title Load data\n",
        "#@title Data Retrieval\n",
        "def process_dataset(data, task):\n",
        "    conditions = []\n",
        "    for c in task:\n",
        "      conditions.append((data.targets == c))\n",
        "    indices = functools.reduce(torch.logical_or, conditions)\n",
        "    data.data, data.targets = data.data[indices], data.targets[indices]\n",
        "    for i in range(len(task)):\n",
        "      data.targets[data.targets == task[i]] = i\n",
        "    data = TensorDataset(data.data.reshape(-1, 28*28), data.targets)\n",
        "    return data\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "\n",
        "# Construct data\n",
        "train_loaders = []\n",
        "MI_loaders    = []\n",
        "test_loaders  = []\n",
        "\n",
        "for taskNum in range(len(tasks)):\n",
        "  train_dataset = process_dataset(datasets.MNIST('../data', download=True, train=True, transform=transform), tasks[taskNum])\n",
        "  MI_dataset    = process_dataset(datasets.MNIST('../data', download=True, train=True, transform=transform), sum(tasks[:taskNum+1], []))\n",
        "  test_dataset  = process_dataset(datasets.MNIST('../data', download=True, train=False, transform=transform), tasks[taskNum])\n",
        "\n",
        "  # use PCA to reduce to lower dimensions\n",
        "  pca = PCA(n_components=pca_components)\n",
        "  pca.fit(train_dataset.tensors[0])\n",
        "  train_dataset.tensors = (torch.from_numpy(pca.transform(train_dataset.tensors[0])).float(), train_dataset.tensors[1])\n",
        "  MI_dataset.tensors    = (torch.from_numpy(pca.transform(MI_dataset.tensors[0])).float(), MI_dataset.tensors[1])\n",
        "  test_dataset.tensors  = (torch.from_numpy(pca.transform(test_dataset.tensors[0])).float(), test_dataset.tensors[1])\n",
        "\n",
        "  # dataset loaders\n",
        "  if pretrain and taskNum == 0:\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "  else:\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
        "\n",
        "  MI_loader_temp = torch.utils.data.DataLoader(\n",
        "      MI_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "  MI_loader = torch.utils.data.DataLoader(\n",
        "      MI_dataset, batch_size=len(MI_loader_temp.dataset), shuffle=False, drop_last=False)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "  test_loader_full = torch.utils.data.DataLoader(\n",
        "      test_dataset, batch_size = len(test_loader.dataset), shuffle=False, drop_last=False)\n",
        "  # save loaders\n",
        "  train_loaders.append(train_loader)\n",
        "  MI_loaders.append(MI_loader)\n",
        "  test_loaders.append(test_loader_full)"
      ],
      "metadata": {
        "id": "8iWWBWetKAQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0192402-d4df-458a-dd5d-02433f92b51c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 39543116.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 97611356.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 38763382.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 19459171.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, numOutput):\n",
        "      super(Model, self).__init__()\n",
        "      self.fc1 = nn.Linear(pca_components, 10)\n",
        "      self.fc2 = nn.Linear(10, 7)\n",
        "      self.fc3 = nn.Linear(7, 5)\n",
        "      self.fc4 = nn.Linear(5, 4)\n",
        "      self.fc5 = nn.Linear(4, 3)\n",
        "      self.is_online = True\n",
        "\n",
        "    def forward(self, x):\n",
        "      h1 = torch.tanh(self.fc1(x))\n",
        "      h2 = torch.tanh(self.fc2(h1))\n",
        "      h3 = torch.tanh(self.fc3(h2))\n",
        "      h4 = torch.tanh(self.fc4(h3))\n",
        "      h5 = torch.tanh(self.fc5(h4))\n",
        "      return h1, h2, h3, h4, h5"
      ],
      "metadata": {
        "id": "4IkfJqxXljJZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model setup\n",
        "model       = Model(numOutput = numOutput).to(device)\n",
        "class_heads = {i : nn.Linear(3, len(tasks[0])).to(device) for i in range(len(tasks))}\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "#@title Pre-Train Model\n",
        "train_accuracy      = []\n",
        "test_set_accuracy   = {}\n",
        "firstOnlineTask     = 0\n",
        "\n",
        "# Mutual information metrics\n",
        "layers              = len(list(model.children())) + 1\n",
        "MI_XH               = torch.zeros(numMI, layers)\n",
        "MI_YH               = torch.zeros(numMI, layers)\n",
        "if pretrain:\n",
        "    print(\"Pre-training on task number 1\")\n",
        "    firstOnlineTask = 1\n",
        "    optimizer       = optim.SGD(model.parameters(),\n",
        "                      lr=lr,\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=weight_decay)\n",
        "\n",
        "    scheduler       = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "    for epoch in range(epochs):\n",
        "        print(\"   Epoch:\", epoch+1, \"/\", epochs)\n",
        "        # train phase\n",
        "        print(\"       Training\")\n",
        "        model.train()\n",
        "        accuracy = 0\n",
        "        N = 0\n",
        "\n",
        "        # iterate over train data\n",
        "        for batch_idx, (images, labels) in enumerate(train_loaders[0], start=1):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            h1, h2, h3, h4, h5 = model(images)\n",
        "            h6 = F.log_softmax(class_heads[0](h5), dim = 1)\n",
        "            loss = loss_function(h6, labels)\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # check if predicted labels are equal to true labels\n",
        "            predicted_labels = torch.argmax(h6,dim=1)\n",
        "            accuracy += torch.sum((predicted_labels==labels).float()).item()\n",
        "            N += images.shape[0]\n",
        "        train_accuracy.append(100. * accuracy/N)\n",
        "        scheduler.step()\n",
        "\n",
        "        # iterate over test data\n",
        "        print(\"       Evaluating test 1 / 1: \", end=\"\")\n",
        "        model.eval()\n",
        "        accuracy = 0\n",
        "        N = 0\n",
        "        for batch_idx, (images, labels) in enumerate(test_loaders[0], start=1):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            h1, h2, h3, h4, h5 = model(images)\n",
        "            h6 = F.log_softmax(class_heads[0](h5), dim = 1)\n",
        "\n",
        "            # check if predicted labels are equal to true labels\n",
        "            predicted_labels = torch.argmax(h6,dim=1)\n",
        "            accuracy += torch.sum((predicted_labels==labels).float()).item()\n",
        "            N += images.shape[0]\n",
        "        test_set_accuracy[(0,0)] = 100. * accuracy/N\n",
        "        print(accuracy, \"/\", N)\n",
        "\n",
        "        for images, labels in MI_loaders[0]:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        h_list = model(images)\n",
        "        h_list = h_list + (F.log_softmax(class_heads[0](h_list[-1]), dim=1), )\n",
        "\n",
        "        # calculate mutual information\n",
        "        print(\"       Computing mutual information\")\n",
        "        for layer, h in enumerate(h_list):\n",
        "            # discretize images and representations\n",
        "            bins = np.linspace(0, 1, n_bins+1)\n",
        "            x_ = np.digitize(images.cpu(), bins)\n",
        "            if layer == layers-1:\n",
        "                h = torch.exp(h)\n",
        "                bins = np.linspace(0, 1, n_bins+1)\n",
        "            else:\n",
        "                bins = np.linspace(-1, 1, n_bins+1)\n",
        "            h_ = np.digitize(h.cpu().detach().numpy(), bins)\n",
        "            y_ = labels.cpu()\n",
        "\n",
        "            # calculate discrete probabilities\n",
        "            p_X = Counter()\n",
        "            p_Y = Counter()\n",
        "            p_H = Counter()\n",
        "            p_XH = Counter()\n",
        "            p_YH = Counter()\n",
        "\n",
        "            for x_i, h_i, y_i in zip(x_, h_, y_):\n",
        "                def totuple(a):\n",
        "                    try:\n",
        "                        return tuple(totuple(i) for i in a)\n",
        "                    except:\n",
        "                        return a.item()\n",
        "\n",
        "                unit = 1./len(images)\n",
        "\n",
        "                p_X[totuple(x_i)]                   += unit\n",
        "                p_Y[totuple(y_i)]                   += unit\n",
        "                p_H[totuple(h_i)]                   += unit\n",
        "                p_XH[(totuple(x_i), totuple(h_i))]  += unit\n",
        "                p_YH[(totuple(y_i), totuple(h_i))]  += unit\n",
        "\n",
        "            for xh in p_XH:\n",
        "                if p_XH[xh] != 0:\n",
        "                    MI_XH[epoch][layer] += p_XH[xh] * math.log2(p_XH[xh] / (p_X[xh[0]] * p_H[xh[1]]))\n",
        "\n",
        "            for yh in p_YH:\n",
        "                if p_YH[yh] != 0:\n",
        "                    MI_YH[epoch][layer] += p_YH[yh] * math.log2(p_YH[yh] / (p_Y[yh[0]] * p_H[yh[1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_ILB8jQxHsm",
        "outputId": "688b12a1-4106-420b-df5d-237dba796250"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training on task number 1\n",
            "   Epoch: 1 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 1135.0 / 2115\n",
            "       Computing mutual information\n",
            "   Epoch: 2 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 1135.0 / 2115\n",
            "       Computing mutual information\n",
            "   Epoch: 3 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 1135.0 / 2115\n",
            "       Computing mutual information\n",
            "   Epoch: 4 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 1903.0 / 2115\n",
            "       Computing mutual information\n",
            "   Epoch: 5 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 2056.0 / 2115\n",
            "       Computing mutual information\n",
            "   Epoch: 6 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 2075.0 / 2115\n",
            "       Computing mutual information\n",
            "   Epoch: 7 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 2080.0 / 2115\n",
            "       Computing mutual information\n",
            "   Epoch: 8 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 2087.0 / 2115\n",
            "       Computing mutual information\n",
            "   Epoch: 9 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 2091.0 / 2115\n",
            "       Computing mutual information\n",
            "   Epoch: 10 / 10\n",
            "       Training\n",
            "       Evaluating test 1 / 1: 2096.0 / 2115\n",
            "       Computing mutual information\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Online Learn\n",
        "for taskNum in range(firstOnlineTask, len(tasks)):\n",
        "    print(\"Task Number:\", taskNum+1, \"/\", len(tasks))\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                      lr=lr_ol[taskNum],\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=weight_decay)\n",
        "\n",
        "    # train phase\n",
        "    print(\"   Training\")\n",
        "    model.train()\n",
        "    accuracy = 0\n",
        "    N = 0\n",
        "\n",
        "    # iterate over train data\n",
        "    for training_sample, (images, labels) in enumerate(train_loaders[taskNum], start=1):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        h1, h2, h3, h4, h5 = model(images)\n",
        "        h6 = F.log_softmax(class_heads[taskNum](h5), dim = 1)\n",
        "        loss = loss_function(h6, labels)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # check if predicted labels are equal to true labels\n",
        "        predicted_labels = torch.argmax(h6,dim=1)\n",
        "        accuracy += torch.sum((predicted_labels==labels).float()).item()\n",
        "        N += images.shape[0]\n",
        "    train_accuracy.append(100. * accuracy/N)\n",
        "\n",
        "    # test phase\n",
        "    model.eval()\n",
        "\n",
        "    # iterate over test data\n",
        "    for testTaskNum in range(taskNum+1):\n",
        "        print(\"   Evaluating test\", testTaskNum+1, \"/\", taskNum+1, end=\": \")\n",
        "        N = 0\n",
        "        accuracy = 0\n",
        "        for batch_idx, (images, labels) in enumerate(test_loaders[testTaskNum], start=1):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            h1, h2, h3, h4, h5 = model(images)\n",
        "            h6 = F.log_softmax(class_heads[taskNum](h5), dim = 1)\n",
        "\n",
        "            # check if predicted labels are equal to true labels\n",
        "            predicted_labels = torch.argmax(h6,dim=1)\n",
        "            accuracy += torch.sum((predicted_labels==labels).float()).item()\n",
        "            N += images.shape[0]\n",
        "        print(accuracy, \"/\", N)\n",
        "        test_set_accuracy[(taskNum, testTaskNum)] = 100. * accuracy/N\n",
        "    # Compute mutual information\n",
        "    print(\"   Getting mutual information\")\n",
        "    # run model on full train dataset\n",
        "    for images, labels in MI_loaders[taskNum]:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    h_list = model(images)\n",
        "    h_list = h_list + (F.log_softmax(class_heads[taskNum](h_list[-1]), dim = 1), )\n",
        "\n",
        "    # calculate mutual information\n",
        "    for layer, h in enumerate(h_list):\n",
        "        # discretize images and representations\n",
        "        bins = np.linspace(0, 1, n_bins+1)\n",
        "        x_ = np.digitize(images.cpu(), bins)\n",
        "        if layer == layers-1:\n",
        "            h = torch.exp(h)\n",
        "            bins = np.linspace(0, 1, n_bins+1)\n",
        "        else:\n",
        "            bins = np.linspace(-1, 1, n_bins+1)\n",
        "        h_ = np.digitize(h.cpu().detach().numpy(), bins)\n",
        "        y_ = labels.cpu()\n",
        "\n",
        "        # calculate discrete probabilities\n",
        "        p_X = Counter()\n",
        "        p_Y = Counter()\n",
        "        p_H = Counter()\n",
        "        p_XH = Counter()\n",
        "        p_YH = Counter()\n",
        "\n",
        "        for x_i, h_i, y_i in zip(x_, h_, y_):\n",
        "            def totuple(a):\n",
        "                try:\n",
        "                    return tuple(totuple(i) for i in a)\n",
        "                except:\n",
        "                    return a.item()\n",
        "\n",
        "            unit = 1./len(images)\n",
        "\n",
        "            p_X[totuple(x_i)]                   += unit\n",
        "            p_Y[totuple(y_i)]                   += unit\n",
        "            p_H[totuple(h_i)]                   += unit\n",
        "            p_XH[(totuple(x_i), totuple(h_i))]  += unit\n",
        "            p_YH[(totuple(y_i), totuple(h_i))]  += unit\n",
        "\n",
        "        for xh in p_XH:\n",
        "          if p_XH[xh] != 0:\n",
        "            MI_XH[epochs+taskNum-1][layer] += p_XH[xh] * math.log2(p_XH[xh] / (p_X[xh[0]] * p_H[xh[1]]))\n",
        "\n",
        "        for yh in p_YH:\n",
        "          if p_YH[yh] != 0:\n",
        "            MI_YH[epochs+taskNum-1][layer] += p_YH[yh] * math.log2(p_YH[yh] / (p_Y[yh[0]] * p_H[yh[1]]))\n",
        "    print(\"     I(X;H) =\", MI_XH[taskNum])\n",
        "    print(\"     I(Y;H) =\", MI_YH[taskNum])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ulv6yE5MAKN",
        "outputId": "89eb3829-19f2-4175-8fd6-30be87dd8076"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task Number: 2 / 5\n",
            "   Training\n",
            "   Evaluating test 1 / 2: 1739.0 / 2115\n",
            "   Evaluating test 2 / 2: 1454.0 / 2042\n",
            "   Getting mutual information\n",
            "     I(X;H) = tensor([ 4.5443e+00,  4.3042e+00,  4.0628e+00,  2.4032e+00,  1.5048e+00,\n",
            "        -1.8258e-13])\n",
            "     I(Y;H) = tensor([ 9.5825e-01,  9.5446e-01,  9.0826e-01,  6.3464e-01,  5.3476e-01,\n",
            "        -1.8260e-13])\n",
            "Task Number: 3 / 5\n",
            "   Training\n",
            "   Evaluating test 1 / 3: 1096.0 / 2115\n",
            "   Evaluating test 2 / 3: 952.0 / 2042\n",
            "   Evaluating test 3 / 3: 971.0 / 1874\n",
            "   Getting mutual information\n",
            "     I(X;H) = tensor([ 4.4361e+00,  4.1962e+00,  3.9355e+00,  2.7854e+00,  1.6665e+00,\n",
            "        -1.8258e-13])\n",
            "     I(Y;H) = tensor([ 9.6181e-01,  9.5777e-01,  9.4930e-01,  8.7732e-01,  8.2012e-01,\n",
            "        -1.8260e-13])\n",
            "Task Number: 4 / 5\n",
            "   Training\n",
            "   Evaluating test 1 / 4: 2105.0 / 2115\n",
            "   Evaluating test 2 / 4: 1713.0 / 2042\n",
            "   Evaluating test 3 / 4: 233.0 / 1874\n",
            "   Evaluating test 4 / 4: 1900.0 / 1986\n",
            "   Getting mutual information\n",
            "     I(X;H) = tensor([4.2673, 4.0406, 3.6831, 2.5628, 1.7396, 0.7470])\n",
            "     I(Y;H) = tensor([0.9658, 0.9622, 0.9506, 0.9315, 0.9109, 0.5594])\n",
            "Task Number: 5 / 5\n",
            "   Training\n",
            "   Evaluating test 1 / 5: 26.0 / 2115\n",
            "   Evaluating test 2 / 5: 230.0 / 2042\n",
            "   Evaluating test 3 / 5: 1653.0 / 1874\n",
            "   Evaluating test 4 / 5: 105.0 / 1986\n",
            "   Evaluating test 5 / 5: 619.0 / 1983\n",
            "   Getting mutual information\n",
            "     I(X;H) = tensor([4.0446, 3.8498, 3.1690, 2.3881, 2.0803, 1.1099])\n",
            "     I(Y;H) = tensor([0.9684, 0.9663, 0.9613, 0.9453, 0.9423, 0.9259])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compute Forgetting Metrics\n",
        "ave_accuracy        = []\n",
        "ave_inc_accuracy    = []\n",
        "forgetting_measure  = []\n",
        "backward_transfer   = []\n",
        "\n",
        "# Compute average accuracy\n",
        "for taskNum in range(len(tasks)):\n",
        "    ave_accuracy.append(0)\n",
        "    for testTaskNum in range(taskNum+1):\n",
        "      ave_accuracy[-1] += test_set_accuracy[(taskNum, testTaskNum)]\n",
        "    ave_accuracy[-1] = ave_accuracy[-1] / (taskNum+1)\n",
        "\n",
        "# Compute average incremental accuracy\n",
        "for taskNum in range(len(tasks)):\n",
        "    ave_inc_accuracy.append(0)\n",
        "    for testTaskNum in range(taskNum+1):\n",
        "      ave_inc_accuracy[-1] += ave_accuracy[testTaskNum]\n",
        "    ave_inc_accuracy[-1] = ave_inc_accuracy[-1] / (taskNum+1)\n",
        "\n",
        "# Compute forgetting measure\n",
        "for taskNum in range(1,len(tasks)):\n",
        "    forgetting_measure.append(0)\n",
        "    for testTaskNum in range(taskNum):\n",
        "      f = -math.inf\n",
        "      for i in range(testTaskNum, taskNum):\n",
        "        if test_set_accuracy[(i, testTaskNum)] > f:\n",
        "          f = test_set_accuracy[(i, testTaskNum)]\n",
        "      forgetting_measure[-1] += f - test_set_accuracy[(taskNum, testTaskNum)]\n",
        "    forgetting_measure[-1] = forgetting_measure[-1] / taskNum\n",
        "\n",
        "# Compute backward transfer\n",
        "for taskNum in range(1, len(tasks)):\n",
        "    backward_transfer.append(0)\n",
        "    for testTaskNum in range(taskNum):\n",
        "      backward_transfer[-1] += test_set_accuracy[(taskNum, testTaskNum)] - test_set_accuracy[(testTaskNum, testTaskNum)]\n",
        "    backward_transfer[-1] = backward_transfer[-1] / taskNum\n",
        "\n",
        "print(\"Average Accuracy:\", ave_accuracy)\n",
        "print(\"Average Incremental Accuracy:\", ave_inc_accuracy)\n",
        "print(\"Forgetting Measure:\", forgetting_measure)\n",
        "print(\"Backward Transfer:\", backward_transfer)"
      ],
      "metadata": {
        "id": "kXnct0URTM-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot Results\n",
        "plt.figure()\n",
        "plt.title(\"Average Accuracy\")\n",
        "plt.plot(ave_accuracy)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Average Incremental Accuracy\")\n",
        "plt.plot(ave_inc_accuracy)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Forgetting Measure\")\n",
        "plt.plot(forgetting_measure)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Backward Transfer\")\n",
        "plt.plot(backward_transfer)\n",
        "\n",
        "plt.figure()\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.set_xlabel('MI_X,H')\n",
        "ax.set_ylabel('MI_Y,H')\n",
        "title = ax.set_title('Information Plane')\n",
        "cmap_dots = plt.get_cmap('gnuplot', len(tasks))\n",
        "dot_colors = [cmap_dots(0)] * epochs + [cmap_dots(i) for i in np.arange(1, len(tasks))]\n",
        "print(dot_colors)\n",
        "#dot_colors = [cmap_dots(i) for i in np.linspace(0, 1, epochs+len(tasks)-1)]\n",
        "cmap_lines = plt.get_cmap('tab10', layers)\n",
        "line_colors = [cmap_lines(i) for i in np.arange(0, layers)]\n",
        "#ax.plot(MI_XH, MI_YH, '-', color='gray')\n",
        "for layer in range(layers):\n",
        "    ax.plot(MI_XH[:,layer], MI_YH[:,layer], '-', color=line_colors[layer], label=\"Layer \" + str(layer+1))\n",
        "    im = ax.scatter(MI_XH[:,layer], MI_YH[:,layer], c=dot_colors)\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "ZScRQWUcbQ7u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}